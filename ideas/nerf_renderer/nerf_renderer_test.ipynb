{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruilongli/anaconda3/envs/tava2/lib/python3.9/site-packages/tqdm-4.64.0-py3.9.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "import raymarching2\n",
    "\n",
    "rays_o = (torch.zeros((2000, 3)) + 0.1).to(\"cuda\")\n",
    "rays_d = torch.randn((2000, 3)).to(\"cuda\")\n",
    "rays_d = F.normalize(rays_d, dim=-1)\n",
    "\n",
    "density_bitfield = (torch.ones(\n",
    "    (5, 128 ** 3 // 8), dtype=torch.uint8\n",
    ") * 255).to(\"cuda\")\n",
    "\n",
    "aabb = torch.tensor([0., 0., 0., 1., 1., 1.]).to(\"cuda\")\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "sigmas = torch.rand((2000, 1024), device=rays_o.device, requires_grad=True)\n",
    "rgbs = torch.rand((2000, 1024, 3), device=rays_o.device, requires_grad=True)\n",
    "bkgd_rgb = torch.rand(3).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 122.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void volumetric_rendering_kernel<float>(unsigned int...         0.00%       0.000us         0.00%       0.000us       0.000us       2.251ms        35.94%       2.251ms       2.251ms             1  \n",
      "void kernel_generate_training_samples<float>(unsigne...         0.00%       0.000us         0.00%       0.000us       0.000us       2.081ms        33.22%       2.081ms       2.081ms             1  \n",
      "void volumetric_rendering_backward_kernel<float>(uns...         0.00%       0.000us         0.00%       0.000us       0.000us       1.863ms        29.74%       1.863ms       1.863ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      36.000us         0.57%      36.000us      18.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      21.000us         0.34%      21.000us       2.100us            10  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.08%       5.000us       1.667us             3  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.08%       5.000us       5.000us             1  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.03%       2.000us       2.000us             1  \n",
      "                                       cudaLaunchKernel         7.84%     460.000us         7.84%     460.000us      23.000us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "                                  cudaDeviceSynchronize        92.16%       5.407ms        92.16%       5.407ms       5.407ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.867ms\n",
      "Self CUDA time total: 6.264ms\n",
      "\n",
      "accumulated_color tensor(3262.9824, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "grad sigmas tensor(-76255.7500, device='cuda:0')\n",
      "grad rgbs tensor(882230.8750, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indices, positions, dirs, deltas, ts = raymarching2.generate_training_samples(\n",
    "    rays_o, rays_d, aabb, density_bitfield, 1024 * rays_o.shape[0]\n",
    ")\n",
    "\n",
    "sigmas_collector = []\n",
    "rgbs_collector = []\n",
    "for ray_id, sample_id, sample_cnt in indices:\n",
    "    sigmas_collector.append(\n",
    "        sigmas[ray_id, 0: sample_cnt]\n",
    "    )\n",
    "    rgbs_collector.append(\n",
    "        rgbs[ray_id, 0: sample_cnt]\n",
    "    )\n",
    "sigmas_collector = torch.cat(sigmas_collector)\n",
    "rgbs_collector = torch.cat(rgbs_collector)\n",
    "\n",
    "sigmas_collector = sigmas_collector.detach().clone().requires_grad_(True)\n",
    "rgbs_collector = rgbs_collector.detach().clone().requires_grad_(True)\n",
    "\n",
    "for _ in tqdm.tqdm(range(1000)):\n",
    "    with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"volumetric_rendering\"):\n",
    "            indices, positions, dirs, deltas, ts = raymarching2.generate_training_samples(\n",
    "                rays_o, rays_d, aabb, density_bitfield, 1024 * rays_o.shape[0]\n",
    "            )\n",
    "            (\n",
    "                accumulated_weight, \n",
    "                accumulated_depth, \n",
    "                accumulated_color, \n",
    "                accumulated_position\n",
    "            ) = raymarching2.volumetric_rendering(\n",
    "                indices, positions, deltas, ts,\n",
    "                sigmas_collector, rgbs_collector,\n",
    "                bkgd_rgb\n",
    "            )\n",
    "            accumulated_color.sum().backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print (prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "print (\"accumulated_color\", accumulated_color.sum())\n",
    "print (\"grad sigmas\", sigmas_collector.grad.sum())\n",
    "print (\"grad rgbs\", rgbs_collector.grad.sum())\n",
    "\n",
    "_ = sigmas_collector.grad.zero_()\n",
    "_ = rgbs_collector.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 116.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_rgb tensor(3263.0479, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "grad sigmas tensor(-75984.8594, device='cuda:0')\n",
      "grad rgbs tensor(882290.8750, device='cuda:0')\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     386.000us        15.16%     386.000us      48.250us             8  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     318.000us        12.49%     318.000us      53.000us             6  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     318.000us        12.49%     318.000us      45.429us             7  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     222.000us         8.72%     222.000us      55.500us             4  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     214.000us         8.40%     214.000us      35.667us             6  \n",
      "std::enable_if<!c10::is_complex<float>::value, void>...         0.00%       0.000us         0.00%       0.000us       0.000us     158.000us         6.20%     158.000us      79.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     155.000us         6.09%     155.000us      25.833us             6  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     142.000us         5.58%     142.000us      35.500us             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      73.000us         2.87%      73.000us      36.500us             2  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      68.000us         2.67%      68.000us      34.000us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.267ms\n",
      "Self CUDA time total: 2.547ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_training_samples(\n",
    "    rays_o: torch.Tensor, \n",
    "    rays_d: torch.Tensor, \n",
    "    aabb: torch.Tensor, \n",
    "    max_samples: int = 10_000,\n",
    "):\n",
    "    device = rays_o.device\n",
    "    NERF_STEPS = 1024\n",
    "    STEPSIZE = 1.7320508075688772 / NERF_STEPS\n",
    "\n",
    "    rays_o = rays_o.reshape(-1, 3)\n",
    "    rays_d = rays_d.reshape(-1, 3)\n",
    "\n",
    "    t_vals = torch.arange(0.0, NERF_STEPS, device=device) * STEPSIZE\n",
    "    t_vals = t_vals[None, :].expand((rays_o.shape[0], -1))\n",
    "    points = rays_o[:, None, :] + rays_d[:, None, :] * t_vals[:, :, None]\n",
    "\n",
    "    selector = (\n",
    "        (points[..., 0] >= aabb[0]) &\n",
    "        (points[..., 1] >= aabb[1]) &\n",
    "        (points[..., 2] >= aabb[2]) &\n",
    "        (points[..., 0] <= aabb[3]) &\n",
    "        (points[..., 1] <= aabb[4]) &\n",
    "        (points[..., 2] <= aabb[5])\n",
    "    )\n",
    "    points[selector]\n",
    "    return points, selector, t_vals\n",
    "\n",
    "\n",
    "def volumetric_rendering(rgb, density, t_vals, dirs, color_bkgd):\n",
    "    \"\"\"Volumetric Rendering Function.\n",
    "    Args:\n",
    "        rgb: torch.ndarray(float32), color, [batch_size, num_samples, 3]\n",
    "        density: torch.ndarray(float32), density, [batch_size, num_samples, 1].\n",
    "        t_vals: torch.ndarray(float32), [batch_size, num_samples].\n",
    "        dirs: torch.ndarray(float32), [batch_size, 3].\n",
    "        color_bkgd: torch.ndarray(float32), [3].\n",
    "    Returns:\n",
    "        comp_rgb: torch.ndarray(float32), [batch_size, 3].\n",
    "        disp: torch.ndarray(float32), [batch_size].\n",
    "        acc: torch.ndarray(float32), [batch_size].\n",
    "        weights: torch.ndarray(float32), [batch_size, num_samples]\n",
    "    \"\"\"\n",
    "    t_dists = torch.cat(\n",
    "        [\n",
    "            t_vals[Ellipsis, 1:] - t_vals[Ellipsis, :-1],\n",
    "            # torch.tensor(\n",
    "            #     [1e10], dtype=t_vals.dtype, device=t_vals.device\n",
    "            # ).expand(t_vals[Ellipsis, :1].shape),\n",
    "            t_vals[Ellipsis, 1:2] - t_vals[Ellipsis, 0:1]\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    delta = t_dists * torch.linalg.norm(dirs[Ellipsis, None, :], dim=-1)\n",
    "    \n",
    "    # Note that we're quietly turning density from [..., 0] to [...].\n",
    "    density_delta = density[..., 0] * delta\n",
    "\n",
    "    alpha = 1 - torch.exp(-density_delta)\n",
    "    trans = torch.exp(\n",
    "        -torch.cat(\n",
    "            [\n",
    "                torch.zeros_like(density_delta[..., :1]),\n",
    "                torch.cumsum(density_delta[..., :-1], dim=-1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "    )\n",
    "    weights = alpha * trans\n",
    "\n",
    "    comp_rgb = (weights[..., None] * rgb).sum(dim=-2)\n",
    "    # print (\"weights\", weights)\n",
    "    acc = weights.sum(dim=-1)\n",
    "    # distance = (weights * t_mids).sum(dim=-1) / acc\n",
    "    # distance = torch.clip(\n",
    "    #     torch.nan_to_num(distance, torch.finfo().max), t_vals[:, 0], t_vals[:, -1]\n",
    "    # )\n",
    "    depth = (weights * t_vals).sum(dim=-1)\n",
    "    eps = 1e-10\n",
    "    inv_eps = 1 / eps\n",
    "    # torch.where accepts <scaler, double tensor>\n",
    "    disp = (acc / depth).double()\n",
    "    disp = torch.where(\n",
    "        (disp > 0) & (disp < inv_eps) & (acc > eps), disp, inv_eps\n",
    "    )\n",
    "    disp = disp.to(acc.dtype)\n",
    "\n",
    "    comp_rgb = comp_rgb + color_bkgd * (1.0 - acc[..., None])\n",
    "    return comp_rgb, depth, acc, weights\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "for _ in tqdm.tqdm(range(1000)):\n",
    "    with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"volumetric_rendering\"):\n",
    "            points, selector, t_vals = generate_training_samples(rays_o, rays_d, aabb)\n",
    "            comp_rgb, depth, acc, weights = volumetric_rendering(\n",
    "                rgbs, (sigmas * selector).unsqueeze(-1), t_vals, \n",
    "                rays_d, bkgd_rgb\n",
    "            )\n",
    "            comp_rgb.sum().backward()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print (\"comp_rgb\", comp_rgb.sum())\n",
    "print (\"grad sigmas\", sigmas.grad.sum())\n",
    "print (\"grad rgbs\", rgbs.grad.sum())\n",
    "# print (rgbs.grad)\n",
    "print (prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "_ = sigmas.grad.zero_()\n",
    "_ = rgbs.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import raymarching2\n",
    "\n",
    "\n",
    "rays_o = (torch.zeros((10, 100, 3)) + 0.1).to(\"cuda\")\n",
    "rays_d = torch.randn((10, 100, 3)).to(\"cuda\")\n",
    "rays_d = F.normalize(rays_d, dim=-1)\n",
    "\n",
    "density_bitfield = (torch.ones(\n",
    "    (5, 128 ** 3 // 8), dtype=torch.uint8\n",
    ") * 255).to(\"cuda\")\n",
    "\n",
    "aabb = torch.tensor([0., 0., 0., 1., 1., 1.]).to(\"cuda\")\n",
    "\n",
    "positions, dirs, deltas, nears, fars = raymarching2.generate_training_samples(\n",
    "    rays_o, rays_d, aabb, density_bitfield\n",
    ")\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tava.utils.plotly import Trimesh, plot_scene, PointCloud\n",
    "\n",
    "def aabb_to_mesh(aabb):\n",
    "    vertices = torch.stack([\n",
    "        aabb[[0, 1, 2]], \n",
    "        aabb[[3, 1, 2]], \n",
    "        aabb[[0, 4, 2]], \n",
    "        aabb[[0, 1, 5]], \n",
    "        aabb[[3, 4, 2]],\n",
    "        aabb[[0, 4, 5]],\n",
    "        aabb[[3, 1, 5]],\n",
    "        aabb[[3, 4, 5]],\n",
    "    ])\n",
    "    faces = torch.tensor([\n",
    "        [0, 1, 4], [0, 4, 2],\n",
    "        [0, 3, 6], [0, 6, 1],\n",
    "        [1, 6, 4], [4, 6, 7],\n",
    "        [2, 4, 7], [2, 7, 5],\n",
    "        [2, 5, 0], [0, 5, 3],\n",
    "        [2, 4, 7], [2, 7, 5],\n",
    "    ], dtype=torch.int32, device=aabb.device)\n",
    "    return vertices, faces\n",
    "\n",
    "vertices, faces = aabb_to_mesh(aabb)\n",
    "plot_scene(\n",
    "    {\n",
    "        \"bbox\": {\n",
    "            \"struct\": Trimesh(\n",
    "                vertices.cpu().numpy(),\n",
    "                faces.cpu().numpy(),\n",
    "            ),\n",
    "            \"mesh_opacity\": 0.7\n",
    "        },\n",
    "        \"samples\": {\n",
    "            \"struct\": PointCloud(\n",
    "                positions[positions.sum(dim=-1) > 0].cpu().numpy()\n",
    "            )\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(positions.sum(dim=-1) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruilongli/anaconda3/envs/tava2/lib/python3.9/site-packages/tqdm-4.64.0-py3.9.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def root_finding(\n",
    "    x: torch.Tensor, \n",
    "    func: Callable, \n",
    "    tol: float = 1e-5, \n",
    "    dvg_thresh: float = 1.0,\n",
    "    max_iters: int = 50,\n",
    "    dtype = torch.float64,  # double is necessary\n",
    "    eps: float = 1e-6,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Root finding for a multidimentional function: f(x) = 0.\n",
    "    \n",
    "    Here we adopt the Broyden's (Bad) Method.\n",
    "    \n",
    "    Args:\n",
    "        x: the initial root of the function. [..., D_in]\n",
    "        func: the callable function that takes x as input. It should\n",
    "            also optionally take a bool as the second argument which\n",
    "            contols where to return the jacobian. The return(s) of \n",
    "            this function is the f(x) or {f(x), J_f(x)}.\n",
    "            The dimentions of the f(x) should be: \n",
    "                {[..., D_in], Optional[bool]} -> \n",
    "                {[..., D_out], Optional[[..., D_in, D_out]]}\n",
    "    Returns:\n",
    "        iters: the number of iterationss being take to finish.\n",
    "        root: the root x being found. torch.FloatTensor [..., D_in]\n",
    "        J: the estimated jacobian. torch.FloatTensor [..., D_in, D_out]\n",
    "        mask: the validity mask of the root. torch.BoolTensor [...,]\n",
    "    \"\"\"\n",
    "    origin_dtype = x.dtype\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        f, J = func(x, mask=None, return_jac=True)\n",
    "\n",
    "    x = x.clone().to(dtype)\n",
    "    f = f.to(dtype)\n",
    "    J = J.to(dtype)\n",
    "\n",
    "    J_inv = J.inverse()\n",
    "    err = torch.linalg.norm(f, dim=-1)\n",
    "    accept_mask = err < tol\n",
    "    reject_mask = err > dvg_thresh\n",
    "    process_mask = ~ (accept_mask | reject_mask)\n",
    "\n",
    "    iters_taken = 0\n",
    "    total_processed = 0\n",
    "    while process_mask.any() and iters_taken < max_iters:\n",
    "        iters_taken += 1\n",
    "        total_processed += process_mask.sum()\n",
    "\n",
    "        # select a slice of data need to be processed.\n",
    "        _x = x[process_mask]\n",
    "        _f = f[process_mask]\n",
    "        _J_inv = J_inv[process_mask]\n",
    "        \n",
    "        # update x\n",
    "        _dx = torch.einsum(\"nij,nj->ni\", _J_inv, _f)  # [N, D_in]\n",
    "        _x_new = _x - _dx  # [N, D_in]\n",
    "        x[process_mask] = _x_new  # [N, D_in]\n",
    "\n",
    "        # update f\n",
    "        _f_new = func(\n",
    "            x.to(origin_dtype), mask=process_mask, return_jac=False\n",
    "        ).to(dtype)[process_mask]  # [N, D_out]\n",
    "        _df = _f_new - _f  # [N, D_out]\n",
    "        f[process_mask] = _f_new  # [N, D_out]\n",
    "\n",
    "        # update J_inv for the next iteration.\n",
    "        _u = torch.einsum(\"nij,nj->ni\", _J_inv, _df)  # [N, D_in]\n",
    "        _d = - 1 * _dx   # [N, D_in]\n",
    "        _a = _d - _u  # [N, D_in]\n",
    "        _b = torch.einsum(\"ni,ni->n\", _d, _u)\n",
    "        _b += (_b > 0) * eps - (_b < 0) * eps\n",
    "        _vT = torch.einsum(\"nij,ni->nj\", _J_inv, _d)\n",
    "        J_inv[process_mask] += torch.einsum(\n",
    "            \"nj,ni->nij\", _vT, _a / _b[:, None]\n",
    "        )  # [N, D_in, D_out] \n",
    "        \n",
    "        err = torch.linalg.norm(f, dim=-1)\n",
    "        accept_mask = err < tol\n",
    "        reject_mask = err > dvg_thresh\n",
    "        process_mask = ~ (accept_mask | reject_mask)\n",
    "\n",
    "        # _err = torch.linalg.norm(_f_new, dim=-1)\n",
    "        # _accept_mask = _err < tol\n",
    "        # _reject_mask = _err > dvg_thresh\n",
    "        # _process_mask = ~ (_accept_mask | _reject_mask)\n",
    "\n",
    "        # J_inv[process_mask] += _dJ_inv * _process_mask[:, None, None]\n",
    "        # err[process_mask] = _err * _process_mask\n",
    "        \n",
    "        # accept_mask = err < tol\n",
    "        # reject_mask = err > dvg_thresh\n",
    "        # process_mask = ~ (accept_mask | reject_mask)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"iter: %d | \" % iters_taken +\n",
    "                \"processing: %.2f%% | \" % (process_mask.float().mean() * 100) +\n",
    "                \"accept: %.2f%% | \" % (accept_mask.float().mean() * 100) +\n",
    "                \"reject: %.2f%% | \" % (reject_mask.float().mean() * 100) +\n",
    "                \"err(min): %.5f | \" % err.min() +\n",
    "                \"err(max): %.5f | \" % err.max() +\n",
    "                \"err(mean): %.5f | \" % err.mean()\n",
    "            )\n",
    "\n",
    "    x = x.to(origin_dtype)\n",
    "    J = J_inv.inverse().to(origin_dtype)\n",
    "    return total_processed, iters_taken, x, J, err, accept_mask  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:16<00:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2364e-07, device='cuda:0') torch.Size([100000, 3]) tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "x_init = torch.rand((100000, 3)).float().to(\"cuda\") * 0.1\n",
    "x_jac = (2 * x_init - 1)[:, None, :] * torch.eye(3).to(x_init)\n",
    "\n",
    "def func(x, mask=None, return_jac=False):\n",
    "    out = x ** 2 - x\n",
    "    if return_jac:\n",
    "        # jac = torch.eye(\n",
    "        #     3, device=x.device\n",
    "        # )[None].expand(x.shape[0], -1, -1)\n",
    "        jac = x_jac\n",
    "        return out, jac\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "\n",
    "for _ in tqdm.tqdm(range(200)):\n",
    "    (\n",
    "        total_processed, iters_taken, x, J, err, accept_mask  \n",
    "    ) = root_finding(x_init, func, verbose=False)\n",
    "    torch.cuda.synchronize()\n",
    "print (x.mean(), x.shape, accept_mask.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _broyden(\n",
    "    g, x_init, J_inv_init, max_steps=50, cvg_thresh=1e-5, dvg_thresh=1, eps=1e-6\n",
    "):\n",
    "    \"\"\"Find roots of the given function g(x) = 0.\n",
    "    This function is impleneted based on https://github.com/locuslab/deq.\n",
    "    Tensor shape abbreviation:\n",
    "        N: number of points\n",
    "        D: space dimension\n",
    "    Args:\n",
    "        g (function): the function of which the roots are to be determined. shape: [N, D, 1]->[N, D, 1]\n",
    "        x_init (tensor): initial value of the parameters. shape: [N, D, 1]\n",
    "        J_inv_init (tensor): initial value of the inverse Jacobians. shape: [N, D, D]\n",
    "        max_steps (int, optional): max number of iterations. Defaults to 50.\n",
    "        cvg_thresh (float, optional): covergence threshold. Defaults to 1e-5.\n",
    "        dvg_thresh (float, optional): divergence threshold. Defaults to 1.\n",
    "        eps (float, optional): a small number added to the denominator to prevent numerical error. Defaults to 1e-6.\n",
    "    Returns:\n",
    "        result (tensor): root of the given function. shape: [N, D, 1]\n",
    "        diff (tensor): corresponding loss. [N]\n",
    "        valid_ids (tensor): identifiers of converged points. [N]\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    x = x_init.clone().detach()\n",
    "    J_inv = J_inv_init.clone().detach()\n",
    "\n",
    "    ids_val = torch.ones(x.shape[0], device=x.device).bool()\n",
    "\n",
    "    gx = g(x, mask=ids_val)\n",
    "    update = -J_inv.bmm(gx)\n",
    "\n",
    "    x_opt = x.clone()\n",
    "    gx_norm_opt = torch.linalg.norm(gx.squeeze(-1), dim=-1)\n",
    "\n",
    "    delta_gx = torch.zeros_like(gx)\n",
    "    delta_x = torch.zeros_like(x)\n",
    "\n",
    "    ids_val = torch.ones_like(gx_norm_opt).bool()\n",
    "\n",
    "    total_processed = 0\n",
    "    for i_step in range(max_steps):\n",
    "        total_processed += ids_val.sum()\n",
    "\n",
    "        # update paramter values\n",
    "        delta_x[ids_val] = update\n",
    "        x[ids_val] += delta_x[ids_val]\n",
    "        delta_gx[ids_val] = g(x, mask=ids_val) - gx[ids_val]\n",
    "        gx[ids_val] += delta_gx[ids_val]\n",
    "\n",
    "        # store values with minial loss\n",
    "        gx_norm = torch.linalg.norm(gx.squeeze(-1), dim=-1)\n",
    "        ids_opt = gx_norm < gx_norm_opt\n",
    "        gx_norm_opt[ids_opt] = gx_norm.clone().detach()[ids_opt]\n",
    "        x_opt[ids_opt] = x.clone().detach()[ids_opt]\n",
    "\n",
    "        print (i_step, gx_norm)\n",
    "\n",
    "        # exclude converged and diverged points from furture iterations\n",
    "        ids_val = (gx_norm_opt > cvg_thresh) & (gx_norm < dvg_thresh)\n",
    "        if ids_val.sum() <= 0:\n",
    "            break\n",
    "\n",
    "        # compute paramter update for next iter\n",
    "        vT = (delta_x[ids_val]).transpose(-1, -2).bmm(J_inv[ids_val])\n",
    "        a = delta_x[ids_val] - J_inv[ids_val].bmm(delta_gx[ids_val])\n",
    "        b = vT.bmm(delta_gx[ids_val])\n",
    "        b[b >= 0] += eps\n",
    "        b[b < 0] -= eps\n",
    "        u = a / b\n",
    "        J_inv[ids_val] += u.bmm(vT)\n",
    "        update = -J_inv[ids_val].bmm(gx[ids_val])\n",
    "\n",
    "\n",
    "        # print (\n",
    "        #     # \"_x_new\", x[ids_val].mean(),\n",
    "        #     # \"_f_new\", gx[ids_val].mean(),\n",
    "        #     # \"err\", gx_norm_opt.mean(),\n",
    "        #     # \"process_mask\", ids_val.float().mean(),\n",
    "        #     \"J_inv\", J_inv.mean(),\n",
    "        #     # \"_b\", b.mean(),\n",
    "        #     # \"_a\", a.mean(),\n",
    "        #     # \"vT\", vT.mean(),\n",
    "        #     \"u\", u.shape, u.mean(),\n",
    "        #     \"vT\", vT.shape, vT.mean(),\n",
    "        # )\n",
    "        # break\n",
    "\n",
    "    iters_taken = i_step + 1\n",
    "    x = x_opt\n",
    "    J = J_inv.inverse()\n",
    "    err = gx_norm_opt\n",
    "    accept_mask = gx_norm_opt <= cvg_thresh\n",
    "\n",
    "    return total_processed, iters_taken, x, J, err, accept_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def root_finding2(\n",
    "    x: torch.Tensor, \n",
    "    func: Callable, \n",
    "    tol: float = 1e-5, \n",
    "    dvg_thresh: float = 1.0,\n",
    "    max_iters: int = 50,\n",
    "    dtype = torch.float64,  # double is necessary\n",
    "    eps: float = 1e-10,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    origin_dtype = x.dtype\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        f, J = func(x, mask=None, return_jac=True)\n",
    "\n",
    "    x = x.clone().to(dtype)\n",
    "    f = f.to(dtype)\n",
    "    J = J.to(dtype)\n",
    "\n",
    "    J_inv = J.inverse()\n",
    "\n",
    "    def _func_g(x, mask = None):\n",
    "        x = x.squeeze(-1)\n",
    "        f = func(\n",
    "            x.to(origin_dtype), \n",
    "            mask=mask.to(origin_dtype) if mask is not None else None, \n",
    "            return_jac=False\n",
    "        ).to(dtype)\n",
    "        f = f[mask].unsqueeze(-1)\n",
    "        return f\n",
    "\n",
    "    x = x.unsqueeze(-1)\n",
    "    total_processed, iters_taken, x, J, err, accept_mask = _broyden(\n",
    "        _func_g, \n",
    "        x, \n",
    "        J_inv, \n",
    "        max_steps=max_iters, \n",
    "        cvg_thresh=tol, \n",
    "        dvg_thresh=dvg_thresh, \n",
    "        eps=eps\n",
    "    )\n",
    "    x = x.squeeze(-1)\n",
    "\n",
    "    x = x.to(origin_dtype)\n",
    "    J = J.to(origin_dtype)\n",
    "    return total_processed, iters_taken, x, J, err, accept_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.0019], device='cuda:0', dtype=torch.float64)\n",
      "1 tensor([8.3125e-05], device='cuda:0', dtype=torch.float64)\n",
      "2 tensor([1.2042e-06], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "x_init = torch.rand((1, 3)).float().to(\"cuda\") * 0.1\n",
    "\n",
    "def func(x, mask=None, return_jac=False):\n",
    "    out = x ** 2 - x\n",
    "    if return_jac:\n",
    "        # jac = torch.eye(\n",
    "        #     3, device=x.device\n",
    "        # )[None].expand(x.shape[0], -1, -1)\n",
    "        jac = (2 * x - 1)[:, None, :] * torch.eye(3).to(x)\n",
    "        return out, jac\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "(\n",
    "    total_processed, iters_taken, x, J, err, accept_mask  \n",
    ") = root_finding2(x_init, func, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6184d9b99cb74e2209cd7e160e4da5ae7659535ee54e7ae300c9d3f329090ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tava2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
