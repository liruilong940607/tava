{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients of input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MLPs.\n",
    "\n",
    "$$ y = mlp(x)$$\n",
    "\n",
    "#### 2. Voxels.\n",
    "\n",
    "$$ y = voxel(x)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from tava.utils.training import learning_rate_decay\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim=3, net_depth=8, net_width=128, skip_layer=4, output_dim=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.net_depth = net_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.input_layers = nn.ModuleList()\n",
    "        in_features = input_dim\n",
    "        for i in range(net_depth):\n",
    "            self.input_layers.append(nn.Linear(in_features, net_width))\n",
    "            if i % skip_layer == 0 and i > 0:\n",
    "                in_features = net_width + input_dim\n",
    "            else:\n",
    "                in_features = net_width\n",
    "        hidden_features = in_features\n",
    "        self.output_layer = nn.Linear(hidden_features, output_dim)\n",
    "        self.net_activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        for i in range(self.net_depth):\n",
    "            x = self.input_layers[i](x)\n",
    "            x = self.net_activation(x)\n",
    "            if i % self.skip_layer == 0 and i > 0:\n",
    "                x = torch.cat([x, inputs], dim=-1)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class Voxel(nn.Module):\n",
    "    def __init__(self, res, bbox, input_dim=3, output_dim=1, init=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        data_shape = [output_dim] + [res] * input_dim\n",
    "        if init is not None:\n",
    "            assert init.shape == torch.Size(data_shape)\n",
    "        else:\n",
    "            init = torch.randn(data_shape)\n",
    "        self.data = nn.Parameter(init.unsqueeze(0))\n",
    "        self.register_buffer(\"bbox\", torch.tensor(bbox))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "        x = torch.sigmoid(x) * 2. - 1.  # [-1, 1]\n",
    "        \n",
    "        # bbox_min, bbox_max = self.bbox.split(split_size=[3, 3], dim=-1)\n",
    "        # x = (x - bbox_min) / (bbox_max - bbox_min) * 2. - 1.  # [-1, 1]\n",
    "        x = x.flip(dims=(-1,))  # the convention is k, j, i\n",
    "\n",
    "        out = F.grid_sample(\n",
    "            self.data,\n",
    "            x.view([1] * self.input_dim + [-1, self.input_dim]),\n",
    "            padding_mode='zeros',\n",
    "            align_corners=True,\n",
    "            mode=\"bicubic\",  # \"bilinear\" | \"bicubic\"\n",
    "        ).transpose(1, -1)\n",
    "        out = out.view(list(x.shape[:-1]) + [self.output_dim])\n",
    "        return out \n",
    "\n",
    "\n",
    "class CNNVoxel(nn.Module):\n",
    "    def __init__(self, res, bbox, latent_dim=1024, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.res = res\n",
    "        self.output_dim = output_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.register_buffer(\"bbox\", torch.tensor(bbox))\n",
    "        self.register_buffer(\"z\", torch.randn(1, latent_dim, 1, 1, 1))\n",
    "        self.layers = nn.ModuleList()\n",
    "        feature_dim = latent_dim\n",
    "        for _ in range(int(math.log2(res))):\n",
    "            self.layers.append(\n",
    "                nn.ConvTranspose3d(feature_dim, feature_dim // 2, 4, 2, 1)\n",
    "            )\n",
    "            feature_dim = feature_dim // 2\n",
    "        self.output_layer = nn.Conv3d(feature_dim, output_dim, 3, 1, 1)\n",
    "\n",
    "    def get_voxel(self):\n",
    "        out = self.z\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            out = F.leaky_relu(out, 0.2)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[-1] == 3, \"only support 3d for now.\"\n",
    "        x = torch.sigmoid(x) * 2. - 1.  # [-1, 1]\n",
    "\n",
    "        data = self.get_voxel()\n",
    "\n",
    "        # bbox_min, bbox_max = self.bbox.split(split_size=[3, 3], dim=-1)\n",
    "        # x = (x - bbox_min) / (bbox_max - bbox_min) * 2. - 1.  # [-1, 1]\n",
    "        x = x.flip(dims=(-1,))  # the convention is k, j, i\n",
    "\n",
    "        out = F.grid_sample(\n",
    "            data,\n",
    "            x.view(1, 1, 1, -1, 3),\n",
    "            padding_mode='zeros',\n",
    "            align_corners=True,\n",
    "        ).transpose(1, -1)\n",
    "        out = out.view(list(x.shape[:-1]) + [self.output_dim])\n",
    "        return out \n",
    "\n",
    "\n",
    "class ListVoxel(nn.Module):\n",
    "    def __init__(self, res, bbox, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.register_buffer(\"bbox\", torch.tensor(bbox))\n",
    "        self.data = nn.ParameterList()\n",
    "        num_levels = int(math.log2(res)) + 1\n",
    "        for _ in range(num_levels):\n",
    "            self.data.append(\n",
    "                nn.Parameter(torch.randn([1, output_dim, res, res, res]))\n",
    "            )\n",
    "            res = res // 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[-1] == 3, \"only support 3d for now.\"\n",
    "        x = torch.sigmoid(x) * 2. - 1.  # [-1, 1]\n",
    "        \n",
    "        # bbox_min, bbox_max = self.bbox.split(split_size=[3, 3], dim=-1)\n",
    "        # x = (x - bbox_min) / (bbox_max - bbox_min) * 2. - 1.  # [-1, 1]\n",
    "        x = x.flip(dims=(-1,))  # the convention is k, j, i\n",
    "\n",
    "        output = torch.zeros(list(x.shape[:-1]) + [self.output_dim]).to(x)\n",
    "        for data in self.data:\n",
    "            out = F.grid_sample(\n",
    "                data,\n",
    "                x.view(1, 1, 1, -1, 3),\n",
    "                padding_mode='zeros',\n",
    "                align_corners=True,\n",
    "            ).transpose(1, -1)\n",
    "            out = out.view(list(x.shape[:-1]) + [self.output_dim])\n",
    "            output = output + out\n",
    "        return output \n",
    "\n",
    "\n",
    "class Engine():\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        target_func, \n",
    "        lr_init,\n",
    "        lr_final, \n",
    "        max_steps, \n",
    "        batch_size,\n",
    "        device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.target_func = target_func\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_final = lr_final\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def run(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr_init)\n",
    "        \n",
    "        pbar = tqdm(range(self.max_steps))\n",
    "        for step in pbar:\n",
    "            lr = learning_rate_decay(\n",
    "                step, self.lr_init, self.lr_final, self.max_steps, \n",
    "            )\n",
    "            for param in optimizer.param_groups:\n",
    "                param[\"lr\"] = lr\n",
    "\n",
    "            x = torch.rand((self.batch_size, self.model.input_dim)).to(self.device)\n",
    "            y = self.model(x)\n",
    "            with torch.no_grad():\n",
    "                target = self.target_func(x)\n",
    "            loss = F.mse_loss(y, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"step {step:07d} lr {lr:.4f}: loss {loss.data:.7f}\")\n",
    "\n",
    "    def optim_input(self, x, x_gt, lr_init, lr_final, max_steps):\n",
    "        x = x.to(self.device)\n",
    "        x_gt = x_gt.to(self.device)\n",
    "        x_opt = x.clone().detach().requires_grad_(True)\n",
    "        with torch.no_grad():\n",
    "            target = self.model(x_gt)\n",
    "                \n",
    "        optimizer = torch.optim.SGD([x_opt], lr=lr_init)\n",
    "        pbar = tqdm(range(max_steps))\n",
    "        for step in pbar:\n",
    "            lr = learning_rate_decay(step, lr_init, lr_final, max_steps)\n",
    "            for param in optimizer.param_groups:\n",
    "                param[\"lr\"] = lr\n",
    "            y = self.model(x_opt)\n",
    "            loss = F.mse_loss(y, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"step {step:07d} lr {lr:.4f}: loss {loss.data:.7f} \"\n",
    "                f\"grad {x_opt.grad.abs().mean().data:.7f} \"\n",
    "                f\"err {F.mse_loss(x_opt, x_gt).data: .7f} \"\n",
    "            )\n",
    "        return x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((128, 2))\n",
    "input_gt = torch.rand((128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "step 0000999 lr 100000.0000: loss 0.0000000 grad 0.0000000 err  0.0000000 : 100%|██████████| 1000/1000 [00:04<00:00, 216.78it/s]\n"
     ]
    }
   ],
   "source": [
    "engine_mlp = Engine(\n",
    "    model=MLP(input_dim=2, output_dim=21),\n",
    "    target_func=lambda x: x,\n",
    "    lr_init=1e-1,\n",
    "    lr_final=1e-2,\n",
    "    max_steps=0,\n",
    "    batch_size=40960,\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "engine_mlp.run()\n",
    "input_opt = engine_mlp.optim_input(input, input_gt, lr_init=1e5, lr_final=1e5, max_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 0001999 lr 10000.0000: loss 0.0000004: 100%|██████████| 2000/2000 [00:07<00:00, 252.00it/s]\n",
      "step 0001999 lr 0.0001: loss 0.0000038 grad 0.0000002 err  0.1613045 : 100%|██████████| 2000/2000 [00:03<00:00, 562.72it/s]\n"
     ]
    }
   ],
   "source": [
    "res = 256\n",
    "bbox = [0, 0, 0, 1, 1, 1]\n",
    "# init = torch.stack(torch.meshgrid(\n",
    "#     torch.linspace(bbox[0], bbox[3], steps=res),\n",
    "#     torch.linspace(bbox[1], bbox[4], steps=res),\n",
    "#     torch.linspace(bbox[2], bbox[5], steps=res),\n",
    "#     indexing=\"ij\",\n",
    "# ))\n",
    "init = None\n",
    "engine_voxel = Engine(\n",
    "    model=Voxel(res=res, bbox=bbox, input_dim=2, output_dim=21, init=init),\n",
    "    target_func=lambda x: engine_mlp.model(x),\n",
    "    lr_init=1e4,\n",
    "    lr_final=1e4,\n",
    "    max_steps=2000,\n",
    "    batch_size=40960,\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "engine_voxel.run()\n",
    "input_opt = engine_voxel.optim_input(input, input_gt, lr_init=1e-1, lr_final=1e-4, max_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.3684, 131.8379],\n",
       "        [122.5303, 161.5197],\n",
       "        [ 39.2880, 248.4836]], device='cuda:1', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_opt[0:3] * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "step 0004999 lr 0.0000: loss 0.0129986 grad 0.0015699 err  0.1554769 valid 1.000: 100%|██████████| 5000/5000 [01:00<00:00, 82.71it/s]\n"
     ]
    }
   ],
   "source": [
    "res = 256\n",
    "bbox = [0, 0, 0, 1, 1, 1]\n",
    "init = None\n",
    "engine_vomlp = Engine(\n",
    "    model=nn.Sequential(\n",
    "        Voxel(res=res, bbox=bbox, output_dim=21, init=init),\n",
    "        MLP(input_dim=21, net_depth=1, output_dim=21),\n",
    "    ),\n",
    "    target_func=lambda x: engine_mlp.model(x),\n",
    "    lr_init=1e1,\n",
    "    lr_final=1e0,\n",
    "    max_steps=0,\n",
    "    batch_size=40960,\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "engine_vomlp.run()\n",
    "input_opt = engine_vomlp.optim_input(input, input_gt, lr_init=1e-3, lr_final=1e-5, max_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = 32\n",
    "# bbox = [0, 0, 0, 1, 1, 1]\n",
    "# engine_cnnvoxel = Engine(\n",
    "#     model=CNNVoxel(res=res, bbox=bbox, output_dim=21),\n",
    "#     target_func=lambda x: engine_mlp.model(x),\n",
    "#     lr_init=1e0,\n",
    "#     lr_final=1e-3,\n",
    "#     max_steps=10000,\n",
    "#     batch_size=40960,\n",
    "#     device=\"cuda:1\",\n",
    "# )\n",
    "# engine_cnnvoxel.run()\n",
    "# input_opt = engine_cnnvoxel.optim_input(input, input_gt, lr_init=1e-2, lr_final=1e-3, max_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 0000000 lr 10.0000: loss 1.2410700: 100%|██████████| 1/1 [00:00<00:00, 39.43it/s]\n",
      "step 0001999 lr 1.0000: loss 0.1599888 grad 0.0000060 err  0.0998197 valid 1.000: 100%|██████████| 2000/2000 [00:05<00:00, 354.79it/s]\n"
     ]
    }
   ],
   "source": [
    "res = 8\n",
    "bbox = [0, 0, 0, 1, 1, 1]\n",
    "engine_voxel = Engine(\n",
    "    model=ListVoxel(res=res, bbox=bbox, output_dim=21),\n",
    "    target_func=lambda x: engine_mlp.model(x),\n",
    "    lr_init=1e1,\n",
    "    lr_final=1e-1,\n",
    "    max_steps=1,\n",
    "    batch_size=40960,\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "engine_voxel.run()\n",
    "input_opt = engine_voxel.optim_input(input, input_gt, lr_init=1e0, lr_final=1e0, max_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1024, 3)).to(device)\n",
    "input_gt = torch.rand((1024, 3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = input.clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([x], lr=1e-3)\n",
    "for i in range(10000):\n",
    "    x = x.to(device)\n",
    "    y = mlp(x)\n",
    "    loss = F.mse_loss(y, y_mlp_gt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print (\"i\", i, \"loss\", loss.data, \"grad\", x.grad.abs().mean(), \"err\", F.mse_loss(x, input_gt).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "voxel = Voxel(res=256, bbox=[0, 0, 0, 1, 1, 1]).to(device)\n",
    "optimizer = torch.optim.SGD(voxel.parameters(), lr=1e5)\n",
    "for i in range(5000):\n",
    "    x = torch.rand((40960, 3)).to(device)\n",
    "    y = voxel(x)\n",
    "    loss = F.mse_loss(y, x)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print (\"i\", i, \"loss\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1024, 3)).to(device)\n",
    "input_gt = torch.rand((1024, 3)).to(device)\n",
    "with torch.no_grad():\n",
    "    y_voxel_gt = voxel(input_gt)\n",
    "    y_mlp_gt = mlp(input_gt)\n",
    "    print ((y_voxel_gt - y_mlp_gt).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = input.clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([x], lr=1e-3)\n",
    "for i in range(10000):\n",
    "    x = x.to(device)\n",
    "    y = mlp(x)\n",
    "    loss = F.mse_loss(y, y_mlp_gt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print (\"i\", i, \"loss\", loss.data, \"grad\", x.grad.abs().mean(), \"err\", F.mse_loss(x, input_gt).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tinycudann as tcnn\n",
    "\n",
    "\n",
    "class TCNNHashPositionalEncoder(nn.Module):\n",
    "    \"\"\" Hash Positinal Encoder from Instant-NGP.\n",
    "    \n",
    "    https://github.com/NVlabs/instant-ngp\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        bounding_box: List[float], \n",
    "        in_dim: int = 3,\n",
    "        n_levels: int = 16,\n",
    "        n_features_per_level: int = 2,\n",
    "        log2_hashmap_size: int = 19,\n",
    "        base_resolution: int = 16,\n",
    "        per_level_scale: float = 2.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # [min_x, min_y, min_z, max_x, max_y, max_z]\n",
    "        self.bounding_box = torch.tensor(bounding_box)\n",
    "        self.in_dim = in_dim\n",
    "        self.n_levels = n_levels\n",
    "        self.n_features_per_level = n_features_per_level\n",
    "        # The input to the tcnn.Encoding should be normalized\n",
    "        # to (0, 1) using `self.bounding_box`\n",
    "        self.encoder = tcnn.Encoding(\n",
    "            n_input_dims=in_dim,\n",
    "            encoding_config={\n",
    "                \"otype\": \"HashGrid\",\n",
    "                \"n_levels\": n_levels,\n",
    "                \"n_features_per_level\": n_features_per_level,\n",
    "                \"log2_hashmap_size\": log2_hashmap_size,\n",
    "                \"base_resolution\": base_resolution,\n",
    "                \"per_level_scale\": per_level_scale,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def out_dim(self):\n",
    "        return self.n_levels * self.n_features_per_level\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :params x: [..., 3],\n",
    "        :return x_enc: [..., self.out_dim]\n",
    "        \"\"\"\n",
    "        bb_min, bb_max = torch.split(\n",
    "            self.bounding_box.to(x), [3, 3], dim=0\n",
    "        )\n",
    "        x = (x - bb_min) / (bb_max - bb_min)\n",
    "        x = self.encoder(\n",
    "            x.reshape(-1, x.shape[-1]).half()\n",
    "        ).to(x).reshape(list(x.shape[:-1]) + [self.out_dim])\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:9\"\n",
    "\n",
    "x_gt = torch.rand((16, 3))\n",
    "encoder_gt = TCNNHashPositionalEncoder([0, 0, 0, 1, 1, 1])\n",
    "out_gt = encoder_gt(x_gt)\n",
    "\n",
    "x = torch.rand((16, 3), requires_grad=True)\n",
    "# encoder = TCNNHashPositionalEncoder([0, 0, 0, 1, 1, 1])\n",
    "# out = encoder(x)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": [x], \"lr\": 1e-2},\n",
    "    {\"params\": encoder_gt.parameters(), \"lr\": 0.0}\n",
    "])\n",
    "for _ in range(10000):    \n",
    "    out = encoder_gt(x)\n",
    "    loss = F.mse_loss(out, out_gt.detach())\n",
    "    err = F.mse_loss(x.detach(), x_gt)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print (\"loss\", loss.data, \"err\", err.data, \"grad\", x.grad.abs().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f156ac58ad89dad164aee97f58386e335ecc1447b9ed93fd42b558e9d07d43f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tava')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
